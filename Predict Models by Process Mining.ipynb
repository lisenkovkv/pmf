{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d36d4c-ea30-4ecb-8e84-0408a8371567",
   "metadata": {},
   "source": [
    "Подключение необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a219769-0f38-493a-b817-0e9ca545daf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pm4py'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpm4py\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pm4py'"
     ]
    }
   ],
   "source": [
    "import pm4py\n",
    "import json\n",
    "import os\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, mean_absolute_error\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from import_data import read_data_equisize, read_data_equitemp, determine_cutoff_point, ActivityPair\n",
    "from forecasting import ARf, ARIMAf, HWf, NAVf, GARCHf\n",
    "from operations import calculate_entropic_relevance\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1636c23f-ca5a-4599-9a6f-0f6dc02dd14e",
   "metadata": {},
   "source": [
    "Инициализация необходимых параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0d9d7-a33d-4a89-9045-fa7d3fcbb591",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Parameters\n",
    "\n",
    "dataset = 'italian'\n",
    "agg_type = 'equisize' # возможен параметр 'equitemp'\n",
    "no_pairs = 0\n",
    "horizon = 25\n",
    "no_intervals = 75\n",
    "no_folds = 10\n",
    "no_intervals_all = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ce669c-5e01-40e6-91f2-8d6d9e34ccf2",
   "metadata": {},
   "source": [
    "Считывание лога"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839171bb-8453-48de-8b22-57bc38b9ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant = xes_importer.Variants.ITERPARSE\n",
    "paras = {variant.value.Parameters.MAX_TRACES: 1000000000}\n",
    "log = xes_importer.apply(dataset + '.xes', parameters=paras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efc948d-3502-4e97-892c-82a07ea63f33",
   "metadata": {},
   "source": [
    "Кодирование активностей в целые числа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0409f0b-a698-4c18-9677-15b27fcf44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_names = pm4py.get_attribute_values(log, 'concept:name')\n",
    "no_act = len(activity_names)\n",
    "act_map = {}\n",
    "reverse_map = {}\n",
    "for a, value in enumerate(activity_names.keys()):\n",
    "    act_map[value] = a\n",
    "    reverse_map[a] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71dd3c6-6845-4b9b-b52a-adca45c671cd",
   "metadata": {},
   "source": [
    "Добавление фиктивных активностей \"start\" и \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8974dec9-ec11-447c-bfbe-5c8c55245f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_map['start'] = no_act\n",
    "act_map['end'] = no_act + 1\n",
    "reverse_map[no_act] = 'start'\n",
    "reverse_map[no_act+1] = 'end'\n",
    "no_act += 2\n",
    "print('Activity encoding:', act_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091d3536-5409-4b91-a88f-288508af2a59",
   "metadata": {},
   "source": [
    "Создание пар активностей существующих переходов внутри путей лога"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac27f35e-0508-46cd-aa32-b91c3849acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "apairs = []\n",
    "for t, trace in enumerate(log):\n",
    "    for e, event in enumerate(trace):\n",
    "        if e == len(trace) - 1:\n",
    "            continue\n",
    "        ap = ActivityPair(event['concept:name'], trace[e+1]['concept:name'], trace[e+1]['time:timestamp'], event, trace[e+1], t)\n",
    "        apairs.append(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89b367-1556-474a-b9b4-4ebd1fcadaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityPair:\n",
    "    def __init__(self, a1, a2, timestamp, event, event2, trace_no):\n",
    "        self.a1 = a1\n",
    "        self.a2 = a2\n",
    "        self.timestamp = timestamp\n",
    "        self.event = event\n",
    "        self.event2 = event2\n",
    "        self.trace_no = trace_no\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.a1 + ' before ' + self.a2 + ' at ' + str(self.timestamp)\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        if self.timestamp > other.timestamp:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78b8071-62f2-45b5-9491-93962cc694f7",
   "metadata": {},
   "source": [
    "Сортировка переходов по времени и вывод их количества на экран"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d38fa-879f-4f4a-9d8d-d26878f20e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_aps = sorted(apairs)\n",
    "print('#DFs:', len(sorted_aps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf4bf1d-e87b-4299-b344-d87a4571d9d8",
   "metadata": {},
   "source": [
    "Разбиение лога на интервалы и нахождение для каждого интервала матрицы числа переходов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456db0c1-1fc4-4d79-8e6d-cd7c71fdeadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating DFG matrix')\n",
    "if agg_type == 'equisize':\n",
    "    dfg_time_matrix_org, interval_timings = read_data_equisize(no_intervals_all, sorted_aps, act_map, log, dataset)\n",
    "else:\n",
    "    dfg_time_matrix_org, interval_timings = read_data_equitemp(no_intervals_all, sorted_aps, act_map, log, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3a2bf-bf22-4176-858a-99d9aec6aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_equisize(no_intervals, sorted_aps, act_map, log, dataset):\n",
    "    interval_width = int(len(sorted_aps) / no_intervals)\n",
    "    no_act = len(act_map.keys())\n",
    "\n",
    "    dfg_time_matrix = np.zeros([no_intervals, no_act, no_act], dtype=int)\n",
    "\n",
    "    interval_timing = []\n",
    "\n",
    "    no_events_sums = 0   # число событий всего в интервале\n",
    "    no_events_logs = 0   # число событий в записанном интервале лога\n",
    "    no_dfs = 0   #\n",
    "\n",
    "    shadow_dict = {}\n",
    "    shadow_event_l = 0\n",
    "    logs_finished = 0\n",
    "    log_progression = {}\n",
    "    for i in range(0, no_intervals): #проходим по каждому индексу интервала\n",
    "        print('Interval ', i+1, '/', no_intervals)\n",
    "        lower_bound = i * interval_width  # номер самой первого перехода из интервала\n",
    "        upper_bound = (i+1) * interval_width # номер самого последнего перехода из интервала\n",
    "        if i == (no_intervals - 1):\n",
    "            upper_bound = len(sorted_aps) # если у нас последний интервал, то последний переход это последняя пара из лога\n",
    "\n",
    "        dfs = sorted_aps[lower_bound:upper_bound] # берем срез переходов ActivePair из текущего интервала\n",
    "        no_dfs += len(dfs) #текущее число обраотанных переходов\n",
    "\n",
    "        print('#DFS:', len(dfs))\n",
    "\n",
    "        empty_mat = np.zeros([no_act, no_act], dtype=float) #создаем пустую матрицу по числу активностей\n",
    "\n",
    "        # For output\n",
    "        filtered_events = {}\n",
    "        start = Event()\n",
    "        end = Event()\n",
    "        start['concept:name'] = str(act_map['start'])\n",
    "        end['concept:name'] = str(act_map['end'])\n",
    "        highest = datetime(1970, 1, 1, tzinfo=pytz.UTC)\n",
    "        lowest = datetime(2050, 1, 1, tzinfo=pytz.UTC)\n",
    "\n",
    "        log_dfs = {}\n",
    "        for df in dfs:  # идем по срезу пар из ActivePair\n",
    "            if df.trace_no not in log_dfs.keys():\n",
    "                log_dfs[df.trace_no] = [] # добавляем все попадающиеся новые трейсы в словарь\n",
    "            log_dfs[df.trace_no].append(df) #добавляем в ключ трейса все его пары переходов ActivePair\n",
    "\n",
    "        for trace_no, dfss in log_dfs.items(): # по порядку вытаскиевам трейс и все его попавшие переходы\n",
    "            # print('\\nTrace:', trace_no)\n",
    "            sorted_dfs = sorted(dfss) # сортируем по времени все переходы в каждом трейсе\n",
    "            filtered_events[trace_no] = []\n",
    "            for df in sorted_dfs: # каждая пара в отсортированном массиве пар текущего трейса\n",
    "                # print(df)\n",
    "                filtered_events[trace_no].append(df.event) # упорядоченные пары добавляются в список\n",
    "                no_events_sums += 1 # число событий в трейсе\n",
    "            filtered_events[trace_no].append(sorted_dfs[len(sorted_dfs) - 1].event2) # так как перед этим добалялись 1ые события из пар,\n",
    "                # то последнее событие оказалось пропущенным, добавялем его (не синтетическое end)\n",
    "\n",
    "            no_events_sums += 1\n",
    "\n",
    "        for trace_no, events in filtered_events.items(): # добавляем синтетические start и end  в каждый трейс\n",
    "            empty_mat[act_map['start'], act_map[events[0]['concept:name']]] += 1\n",
    "            empty_mat[act_map[events[-1]['concept:name']], act_map['end']] += 1\n",
    "\n",
    "        # Export filtered events to interval event logs\n",
    "        new_log = EventLog()\n",
    "        no_eve = 0\n",
    "        for t, trace in enumerate(log): # идем по первоначальному логу по трейсам\n",
    "            if i < 50: # странная ситуевина, видимо что-то вспомогательное, не обращаем внимание\n",
    "                if t not in shadow_dict.keys():\n",
    "                    shadow_dict[t] = []\n",
    "            new_trace = Trace()\n",
    "            for trace_no, events in filtered_events.items(): # извлекаем из списка с путями номер трейса(индекс списка) и список событий\n",
    "                if t == trace_no: # если номер трейса изначального лога совпал с номером трейса отфильтрованного лога\n",
    "                    for event in trace: # то для каждого события в первоначальном трейсе\n",
    "                        if event in events: # если событие из первоначального трейса попало в разделение по интервалам\n",
    "                            if event['time:timestamp'] < lowest: # отслеживаем минимум времени интервала и максимум\n",
    "                                lowest = event['time:timestamp']\n",
    "                            if event['time:timestamp'] > highest:\n",
    "                                highest = event['time:timestamp']\n",
    "                            new_event = Event() # создаем событие\n",
    "                            new_event['concept:name'] = str(act_map[event['concept:name']]) # преобразуем название в число\n",
    "                            new_trace.append(new_event) # добавляем событие в трейс\n",
    "                            no_events_logs += 1 # инкрементируем число событий в интервале лога\n",
    "                            no_eve += 1\n",
    "                            if i < 50:\n",
    "                                shadow_dict[t].append(new_event)\n",
    "                                shadow_event_l += 1\n",
    "            if len(new_trace) > 0:\n",
    "                # new_trace.append(end)\n",
    "                new_log.append(new_trace)\n",
    "\n",
    "        exporter.apply(new_log, './logs/' + dataset + '_log_interval_' + str(i) + '-'\n",
    "                       + str(no_intervals) + '_equisize.xes')\n",
    "\n",
    "        print('#Events:', no_eve)\n",
    "        print('#Events log:', no_events_logs)\n",
    "        print('#Events shadow low:', shadow_event_l)\n",
    "        for act_pair in dfs: # расчитываем матрицу переходов для текущего интервала\n",
    "            a1 = act_map[act_pair.a1]\n",
    "            a2 = act_map[act_pair.a2]\n",
    "            empty_mat[a1, a2] += 1\n",
    "\n",
    "        dfg_time_matrix[i] = empty_mat # добавляем в индекс интервала соответствующую ему матрицу переходов\n",
    "\n",
    "        interval_timing.append((lowest, highest))\n",
    "\n",
    "    print('Event sums:', no_events_sums)\n",
    "    print('Event logs:', no_events_logs)\n",
    "    print('#DFS:', no_dfs)\n",
    "    print('Logs finished:', logs_finished)\n",
    "    return dfg_time_matrix, interval_timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d877f30-a954-4ac6-afa6-04d27c1bb439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_equitemp(no_intervals, sorted_aps, act_map, log, dataset):\n",
    "    timestamps = pm4py.get_attribute_values(log, 'time:timestamp')\n",
    "    print('Earliest:', min(timestamps))\n",
    "    print('Latest:', max(timestamps))\n",
    "    interval_length = (max(timestamps) - min(timestamps)) / no_intervals\n",
    "    print('Interval length:', interval_length)\n",
    "\n",
    "    no_act = len(act_map.keys())\n",
    "\n",
    "    dfg_time_matrix = np.zeros([no_intervals, no_act, no_act], dtype=int)\n",
    "\n",
    "    interval_timing = []\n",
    "    no_events_sums = 0\n",
    "    no_events_logs = 0\n",
    "    no_dfs = 0\n",
    "    for i in range(0, no_intervals):\n",
    "        print('Interval ', i, '/', no_intervals)\n",
    "        lower_bound = min(timestamps) + i * interval_length\n",
    "        if i == (no_intervals - 1):\n",
    "            upper_bound = min(timestamps) + (i + 1) * interval_length * 2\n",
    "        else:\n",
    "            upper_bound = min(timestamps) + (i + 1) * interval_length\n",
    "        lb = lower_bound\n",
    "        ub = upper_bound\n",
    "        print(lb)\n",
    "        print(ub)\n",
    "\n",
    "        dfs = []\n",
    "        empty_mat = np.zeros([no_act, no_act], dtype=float)\n",
    "\n",
    "        filtered_events = {}\n",
    "        start = Event()\n",
    "        end = Event()\n",
    "        start['concept:name'] = str(act_map['start'])\n",
    "        end['concept:name'] = str(act_map['end'])\n",
    "        highest = datetime(1970, 1, 1, tzinfo=pytz.UTC)\n",
    "        lowest = datetime(2050, 1, 1, tzinfo=pytz.UTC)\n",
    "\n",
    "        count = 0\n",
    "        for df in sorted_aps:\n",
    "            if ub > df.event2['time:timestamp'] >= lb:# and ub > df.event['time:timestamp'] >= lb:\n",
    "                dfs.append(df)\n",
    "\n",
    "        no_dfs += len(dfs)\n",
    "\n",
    "        log_dfs = {}\n",
    "        for df in dfs:\n",
    "            if df.trace_no not in log_dfs.keys():\n",
    "                log_dfs[df.trace_no] = []\n",
    "            log_dfs[df.trace_no].append(df)\n",
    "\n",
    "        for trace_no, dfss in log_dfs.items():\n",
    "            # print('\\nTrace:', trace_no)\n",
    "            sorted_dfs = sorted(dfss)\n",
    "            filtered_events[trace_no] = []\n",
    "            for df in sorted_dfs:\n",
    "                # print(df)\n",
    "                filtered_events[trace_no].append(df.event)\n",
    "                no_events_sums += 1\n",
    "            filtered_events[trace_no].append(sorted_dfs[len(sorted_dfs)-1].event2)\n",
    "            no_events_sums += 1\n",
    "\n",
    "        print('#traces:', len(log_dfs))\n",
    "\n",
    "        for trace_no, events in filtered_events.items():\n",
    "            empty_mat[act_map['start'], act_map[events[0]['concept:name']]] += 1\n",
    "            empty_mat[act_map[events[-1]['concept:name']], act_map['end']] += 1\n",
    "\n",
    "        # Export filtered events to interval event logs\n",
    "        new_log = EventLog()\n",
    "        no_eve = 0\n",
    "        for t, trace in enumerate(log):\n",
    "            new_trace = Trace()\n",
    "            # new_trace.append(start)\n",
    "            for trace_no, events in filtered_events.items():\n",
    "                if t == trace_no:\n",
    "                    for event in trace:\n",
    "                        if event in events:\n",
    "                            if event['time:timestamp'] < lowest:\n",
    "                                lowest = event['time:timestamp']\n",
    "                            if event['time:timestamp'] > highest:\n",
    "                                highest = event['time:timestamp']\n",
    "                            new_event = Event()\n",
    "                            new_event['concept:name'] = str(act_map[event['concept:name']])\n",
    "                            new_trace.append(new_event)\n",
    "                            no_events_sums += 1\n",
    "                            no_eve += 1\n",
    "            if len(new_trace) > 0:\n",
    "                # new_trace.append(end)\n",
    "                new_log.append(new_trace)\n",
    "        exporter.apply(new_log, './logs/' + dataset + '_log_interval_' + str(i) + '-'\n",
    "                       + str(no_intervals) + '_equitemp.xes')\n",
    "\n",
    "        # print('no eve:', no_eve)\n",
    "        for act_pair in dfs:\n",
    "            a1 = act_map[act_pair.a1]\n",
    "            a2 = act_map[act_pair.a2]\n",
    "            empty_mat[a1, a2] += 1\n",
    "\n",
    "        dfg_time_matrix[i] = empty_mat\n",
    "        interval_timing.append((lowest, highest))\n",
    "    print('Event sums:', no_events_sums)\n",
    "    print('Event logs:', no_events_logs)\n",
    "    print('#DFS:', no_dfs)\n",
    "\n",
    "    return dfg_time_matrix, interval_timing\n",
    "\n",
    "\n",
    "def determine_cutoff_point(act_map, dfg_time_matrix, no_pairs):\n",
    "    counts = []\n",
    "    for act, a in act_map.items():\n",
    "        for act2, a2 in act_map.items():\n",
    "            counts.append(np.sum(dfg_time_matrix[:, a, a2]))\n",
    "    fc = Counter(counts)\n",
    "    no_found = 0\n",
    "    cutoff = 0\n",
    "    for sum in sorted(fc.keys(), reverse=True):\n",
    "        cutoff = sum\n",
    "        no_found += fc[sum]\n",
    "        if no_found > no_pairs:\n",
    "            break\n",
    "    print('CUTOFF:', cutoff, '#pairs:', no_found)\n",
    "\n",
    "    return cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc7dee5-5414-47ef-a172-96fa7c68078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'dfg_time_matrix_{dataset}_{agg_type}.npy', 'wb') as f:\n",
    "    np.save(f, dfg_time_matrix_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd73331-0497-4efe-aa1a-459a0f7bd73d",
   "metadata": {},
   "source": [
    "Нахождение величины числа переходов, чтобы оставить наиболее частые относительно числа отображаемых переходов заданных в параметры no_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6102e8ca-ae0a-4e34-b759-14b57394748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if no_pairs == 0:\n",
    "    cutoff = 0\n",
    "else:\n",
    "    cutoff = determine_cutoff_point(act_map, dfg_time_matrix_org, no_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce3975-91da-4fcb-a766-35c572928dee",
   "metadata": {},
   "source": [
    "Обрезка матрицы переходов берем no_intervals число интервалов из no_intervals_all интервалов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c78c50-76e2-4f0e-8651-5820605706ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce matrix according to parameter settings\n",
    "dfg_time_matrix = dfg_time_matrix_org[:no_intervals, ::, ::]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1759c8d-b404-4cc6-a5a7-e2b416353645",
   "metadata": {},
   "source": [
    "Инициализация переменных для хранения результатов предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0589e1-448e-43f8-b47e-85daeea49bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "techniques = ['nav', 'ar1', 'ar2', 'ar4', 'arima211', 'arima212', 'hw', 'garch']\n",
    "\n",
    "dfg_result_matrix = {}\n",
    "dfg_actual_matrix = np.zeros([no_act, no_act, no_folds, horizon], dtype=int)\n",
    "\n",
    "for technique in techniques:\n",
    "    dfg_result_matrix[technique] = np.zeros([no_act, no_act, no_folds, horizon], dtype=float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99216774-7e80-4857-834c-71489473a47e",
   "metadata": {},
   "source": [
    "Классы обертки для преддиктивных моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6cef32-233e-4d1d-99b7-b61771bb2341",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAVf:\n",
    "\n",
    "    class Fitted:\n",
    "\n",
    "        def __init__(self, x):\n",
    "            self.x = x\n",
    "\n",
    "        def forecast(self, horizon):\n",
    "            mean = np.mean(self.x)\n",
    "            # print('Naive:', np.full(shape=(1, horizon), fill_value=mean))\n",
    "            return np.full(shape=(1, horizon), fill_value=mean)\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, x, horizon):\n",
    "        return self.Fitted(x).forecast(horizon)[0]\n",
    "\n",
    "\n",
    "class ARf:\n",
    "\n",
    "    def __init__(self, d):\n",
    "        self.d = d\n",
    "\n",
    "    def fit(self, x, horizon):\n",
    "        fit = AR(x, lags=self.d, old_names=False).fit()\n",
    "        point_forecast = fit.get_prediction(len(x), len(x)+horizon-1)\n",
    "\n",
    "        # print(f'AR {self.d}:', point_forecast.predicted_mean)\n",
    "        # print(point_forecast.conf_int(0.05))\n",
    "\n",
    "        return point_forecast.predicted_mean\n",
    "\n",
    "\n",
    "class ARIMAf:\n",
    "\n",
    "    def __init__(self, p, d, q):\n",
    "        self.p = p\n",
    "        self.d = d\n",
    "        self.q = q\n",
    "\n",
    "    def fit(self, x, horizon):\n",
    "        fit = ARIMA(x, order=(self.p, self.d, self.q)).fit()\n",
    "        point_forecast = fit.get_forecast(horizon)\n",
    "        conf_int = fit.conf_int()\n",
    "\n",
    "        #print('ARIMA:', point_forecast.predicted_mean)\n",
    "        #print(point_forecast.conf_int())\n",
    "        return point_forecast.predicted_mean\n",
    "\n",
    "class HWf:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, x, horizon):\n",
    "        fit = HW(x, initialization_method='estimated').fit()\n",
    "\n",
    "        # print('HW', fit.forecast(horizon))\n",
    "        return fit.forecast(horizon)\n",
    "\n",
    "\n",
    "class GARCHf:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, x, horizon):\n",
    "        garch = ARCH(x)\n",
    "        model = garch.fit(disp='off')\n",
    "        y_hat = model.forecast(horizon=horizon)\n",
    "\n",
    "        pred = y_hat.mean.iloc[[-1]].values[0]\n",
    "        var = np.sqrt(y_hat.variance.iloc[[-1]].values) * 1.96\n",
    "        conf_low = pred + var[0]\n",
    "        conf_high = pred - var[0]\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620c76da-5714-4394-aaca-74d5c08ea63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Обучение на интервалах, получение предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c6c892-3cd7-4055-8155-671581a53425",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_pairs = set()\n",
    "# forecast DFs of all activity pairs\n",
    "for act, a in act_map.items():\n",
    "    for act_2, a2 in act_map.items():\n",
    "\n",
    "        # by default only time series with at least 1 DF will be selected\n",
    "        if np.sum(dfg_time_matrix[:, a, a2]) > cutoff:\n",
    "            print('DFG', act, 'and', act_2)\n",
    "            chosen_pairs.add((a, a2))\n",
    "\n",
    "            # get DF\n",
    "            array = dfg_time_matrix[:, a, a2] # срез числа появлений пары событий в каждом интервале\n",
    "\n",
    "            techniques = dict()\n",
    "            techniques['nav'] = NAVf()\n",
    "            # techniques['ar1'] = ARf(1)\n",
    "            techniques['ar2'] = ARf(2)\n",
    "            techniques['ar4'] = ARf(4)\n",
    "            # techniques['arima211'] = ARIMAf(2, 1, 1)\n",
    "            techniques['arima212'] = ARIMAf(2, 1, 2)\n",
    "            techniques['hw'] = HWf()\n",
    "            techniques['garch'] = GARCHf()\n",
    "\n",
    "            # cross-validation is applied\n",
    "            for fold in range(0, no_folds):\n",
    "                # offset for cross-validation\n",
    "                offset = - fold - horizon\n",
    "                x = array[:offset] # обрезаем срез числа появлений на обучающую и тестовую выборку\n",
    "\n",
    "                if fold == 0:\n",
    "                    y = array[offset:]\n",
    "                else:\n",
    "                    y = array[offset:(offset + horizon)]\n",
    "\n",
    "                # store actual\n",
    "                dfg_actual_matrix[a, a2, fold] = y\n",
    "\n",
    "                for technique, implement in techniques.items():\n",
    "                    y_pred = []\n",
    "                    my_x = np.copy(x)\n",
    "                    ######\n",
    "                    print('Массив для обучения для вершин ', a, a2)\n",
    "                    print(type(my_x))\n",
    "                    print(my_x)\n",
    "                    ######\n",
    "                    try:\n",
    "                        # predict horizon steps ahead\n",
    "                        y_hat = implement.fit(my_x, horizon)\n",
    "                        y_pred = y_hat\n",
    "                        dfg_result_matrix[technique][a, a2, fold] = y_pred\n",
    "                    except:\n",
    "                        dfg_result_matrix[technique][a, a2, fold] = np.full((horizon, ), 100000000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a505092-80be-47e9-bcb9-2979bf90d1d1",
   "metadata": {},
   "source": [
    "Подключение работы библиотеки для расчета энтропии результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd157e-d83e-4fc6-928e-f39b9be0e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropic_relevance(log_name, dfg_name):\n",
    "    try:\n",
    "        args = []\n",
    "        args.append('java')  # Запуск Java\n",
    "        args.append('-cp')  # Указание classpath\n",
    "        # Указываем все необходимые JAR-файлы через точку с запятой (для Windows)\n",
    "        args.append('jbpt-pm-entropia-1.6.jar;commons-cli-1.9.0.jar')\n",
    "        args.append('org.jbpt.pm.tools.QualityMeasuresCLI')  # Указываем основной класс\n",
    "        args.append('-r')  # Параметры для основного класса\n",
    "        args.append('-s')\n",
    "        args.append('-rel')\n",
    "        logstring = log_name + '.xes'  # Имя лог-файла\n",
    "        args.append(logstring)\n",
    "        predstring = dfg_name + '.json'  # Имя JSON-файла\n",
    "        args.append('-ret')\n",
    "        args.append(predstring)\n",
    "\n",
    "        # Запуск команды и получение результата\n",
    "        result = subprocess.check_output(args)\n",
    "        result_s = float(result)  # Преобразуем результат в число\n",
    "    except Exception as e:\n",
    "        result_s = 'NA'\n",
    "        print(\"Error:\", e)  # Вывод ошибки, если она возникла\n",
    "\n",
    "    return result_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17090bbc-bc66-49ce-bbfd-1ce347e06f59",
   "metadata": {},
   "source": [
    "Оценка полученных результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fedd275-4933-4366-beff-401808606ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results_{dataset}_nopairs_{no_pairs}_nointervals_{str(no_intervals)}_{agg_type}.csv', 'w') as technique_fold_results:\n",
    "    technique_fold_results.write('intervals,technique,fold,horizon,cosine,rmse,er_pred,er_actual\\n')\n",
    "\n",
    "    for technique in techniques:\n",
    "        print(f'Technique {technique}')\n",
    "        dfg_result_matrix_ar = dfg_result_matrix[technique]\n",
    "        results_selected = np.zeros((len(chosen_pairs), no_folds, horizon))\n",
    "        actual_selected = np.zeros((len(chosen_pairs), no_folds, horizon))\n",
    "\n",
    "        for p, pair in enumerate(chosen_pairs):\n",
    "            results_selected[p] = dfg_result_matrix_ar[pair[0], pair[1], ::, ::]\n",
    "            actual_selected[p] = dfg_actual_matrix[pair[0], pair[1], ::, ::]\n",
    "\n",
    "        for h in range(0, horizon):\n",
    "            for fold in range(0, no_folds):\n",
    "                # print(f'Fold {fold} - horizon {h}')\n",
    "\n",
    "                #####################\n",
    "                # Forecast DFG output\n",
    "                nodes = []\n",
    "                for act_name, act_code in act_map.items():\n",
    "                    out_freq = int(np.sum(dfg_result_matrix_ar[act_code, ::, fold, h]))\n",
    "                    in_freq = int(np.sum(dfg_result_matrix_ar[::, act_code, fold, h]))\n",
    "\n",
    "                    if act_name == 'start':\n",
    "                        nodes.append({'label': str(act_code), 'freq': out_freq, 'id': act_code})\n",
    "                    else:\n",
    "                        if in_freq > 0:\n",
    "                            nodes.append({'label': str(act_code), 'freq': in_freq, 'id': act_code})\n",
    "\n",
    "                arcs = []\n",
    "                for a in range(0, len(act_map)):\n",
    "                    for a2 in range(0, len(act_map)):\n",
    "                        if int(dfg_result_matrix_ar[a, a2, fold, h]) > 0:\n",
    "                            arcs.append({'from': a, 'to': a2, 'freq': int(dfg_result_matrix_ar[a, a2, fold, h])})\n",
    "\n",
    "                # calculate entropic relevance\n",
    "                dfg_file = {'nodes': nodes, 'arcs': arcs}\n",
    "                r = json.dumps(dfg_file, indent=1)\n",
    "                with open(f'temp.json', 'w') as dfg_write_file:\n",
    "                    dfg_write_file.write(r)\n",
    "\n",
    "                offset = no_intervals - horizon + h - fold\n",
    "                er_technique = calculate_entropic_relevance(f'./logs/{dataset}_log_interval_{offset}-{no_intervals_all}_{agg_type}', 'temp')\n",
    "\n",
    "\n",
    "                ###################\n",
    "                # Actual DFG output\n",
    "                nodes = []\n",
    "                for act_name, act_code in act_map.items():\n",
    "                    out_freq = int(np.sum(dfg_actual_matrix[act_code, ::, fold, h]))\n",
    "                    in_freq = int(np.sum(dfg_actual_matrix[::, act_code, fold, h]))\n",
    "\n",
    "                    if act_name == 'start':\n",
    "                        nodes.append({'label': str(act_code), 'freq': out_freq, 'id': act_code})\n",
    "                    else:\n",
    "                        if in_freq > 0:\n",
    "                            nodes.append({'label': str(act_code), 'freq': in_freq, 'id': act_code})\n",
    "\n",
    "                arcs = []\n",
    "                for a in range(0, len(act_map)):\n",
    "                    for a2 in range(0, len(act_map)):\n",
    "                        if int(dfg_actual_matrix[a, a2, fold, h]) > 0:\n",
    "                            arcs.append({'from': a, 'to': a2, 'freq': int(dfg_actual_matrix[a, a2, fold, h])})\n",
    "\n",
    "                # calculate entropic relevance\n",
    "                dfg_file = {'nodes': nodes, 'arcs': arcs}\n",
    "                r = json.dumps(dfg_file, indent=1)\n",
    "                with open(f'temp.json', 'w') as dfg_write_file:\n",
    "                    dfg_write_file.write(r)\n",
    "\n",
    "                offset = no_intervals - horizon + h - fold\n",
    "                er_actual = calculate_entropic_relevance(f'./logs/{dataset}_log_interval_{offset}-{no_intervals_all}_{agg_type}', 'temp')\n",
    "\n",
    "                # store results\n",
    "                results = np.reshape(results_selected[::, fold, h], (1, len(chosen_pairs)))\n",
    "                actuals = np.reshape(actual_selected[::, fold, h], (1, len(chosen_pairs)))\n",
    "\n",
    "                cosine = distance.cosine(results, actuals)\n",
    "                rmse = sqrt(mean_squared_error(actuals, results))\n",
    "\n",
    "                technique_fold_results.write(f'{no_intervals},{technique},{str(fold)},{str(h)},{cosine},{rmse},{er_technique},{er_actual}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
